{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 11: TensorFlow (workshop)\n",
    "### Presented by [Jeff Heaton, RGA](https://www.rgare.com/knowledge-center/media/articles/rga-where-logic-meets-curiosity)\n",
    "For more examples like these, refer to my [class website](https://sites.wustl.edu/jeffheaton/t81-558/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 12:04:33) \n",
      "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]\n"
     ]
    }
   ],
   "source": [
    "# What version of Python do you have?\n",
    "\n",
    "import sys\n",
    "print (sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jheaton/miniconda3/envs/rga/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.0\n"
     ]
    }
   ],
   "source": [
    "# Do you have TensorFlow installed?\n",
    "\n",
    "import tensorflow\n",
    "\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow as a Computation Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[12.]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# Very simple calculation.\n",
    "\n",
    "import tensorflow as tf \n",
    "\n",
    "matrix1 = tf.constant([[3., 3.]]) \n",
    "matrix2 = tf.constant([[2.],[2.]]) \n",
    "product = tf.matmul(matrix1, matrix2) \n",
    "\n",
    "with tf.Session() as sess: \n",
    "    result = sess.run([product]) \n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2. -1.]\n",
      "[1. 3.]\n"
     ]
    }
   ],
   "source": [
    "# With variables.\n",
    "\n",
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "x = tf.Variable([1.0, 2.0])\n",
    "a = tf.constant([3.0, 3.0])\n",
    "x.initializer.run()\n",
    "\n",
    "sub = tf.subtract(x, a)\n",
    "print(sub.eval())\n",
    "\n",
    "sess.run(x.assign([4.0, 6.0])) \n",
    "print(sub.eval())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mandelbrot in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import libraries for simulation\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Imports for visualization\n",
    "import PIL.Image\n",
    "from io import BytesIO\n",
    "from IPython.display import Image, display\n",
    "\n",
    "def DisplayFractal(a, fmt='jpeg'):\n",
    "  \"\"\"Display an array of iteration counts as a\n",
    "     colorful picture of a fractal.\"\"\"\n",
    "  a_cyclic = (6.28*a/20.0).reshape(list(a.shape)+[1])\n",
    "  img = np.concatenate([10+20*np.cos(a_cyclic),\n",
    "                        30+50*np.sin(a_cyclic),\n",
    "                        155-80*np.cos(a_cyclic)], 2)\n",
    "  img[a==a.max()] = 0\n",
    "  a = img\n",
    "  a = np.uint8(np.clip(a, 0, 255))\n",
    "  f = BytesIO()\n",
    "  PIL.Image.fromarray(a).save(f, fmt)\n",
    "  display(Image(data=f.getvalue()))\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Use NumPy to create a 2D array of complex numbers\n",
    "\n",
    "Y, X = np.mgrid[-1.3:1.3:0.005, -2:1:0.005]\n",
    "Z = X+1j*Y\n",
    "\n",
    "xs = tf.constant(Z.astype(np.complex64))\n",
    "zs = tf.Variable(xs)\n",
    "ns = tf.Variable(tf.zeros_like(xs, tf.float32))\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# Compute the new values of z: z^2 + x\n",
    "zs_ = zs*zs + xs\n",
    "\n",
    "# Have we diverged with this new value?\n",
    "not_diverged = tf.abs(zs_) < 4\n",
    "\n",
    "step = tf.group(\n",
    "  zs.assign(zs_),\n",
    "  ns.assign_add(tf.cast(not_diverged, tf.float32))\n",
    "  )\n",
    "\n",
    "for i in range(200): step.run()\n",
    "    \n",
    "DisplayFractal(ns.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpful Functions for Keras and TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "\n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name, x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Encode text values to a single dummy variable.  The new columns (which do not replace the old) will have a 1\n",
    "# at every location where the original column (name) matches each of the target_values.  One column is added for\n",
    "# each target value.\n",
    "def encode_text_single_dummy(df, name, target_values):\n",
    "    for tv in target_values:\n",
    "        l = list(df[name].astype(str))\n",
    "        l = [1 if str(x) == str(tv) else 0 for x in l]\n",
    "        name2 = \"{}-{}\".format(name, tv)\n",
    "        df[name2] = l\n",
    "\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "\n",
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name] - mean) / sd\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the median\n",
    "def missing_median(df, name):\n",
    "    med = df[name].median()\n",
    "    df[name] = df[name].fillna(med)\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the default\n",
    "def missing_default(df, name, default_value):\n",
    "    df[name] = df[name].fillna(default_value)\n",
    "\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column.  Is it really this hard? :(\n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if hasattr(target_type, '__iter__') else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df.as_matrix(result).astype(np.float32), dummies.as_matrix().astype(np.float32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df.as_matrix(result).astype(np.float32), df.as_matrix([target]).astype(np.float32)\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "\n",
    "# Regression chart.\n",
    "def chart_regression(pred,y,sort=True):\n",
    "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
    "    if sort:\n",
    "        t.sort_values(by=['y'],inplace=True)\n",
    "    a = plt.plot(t['y'].tolist(),label='expected')\n",
    "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def remove_outliers(df, name, sd):\n",
    "    drop_rows = df.index[(np.abs(df[name] - df[name].mean()) >= (sd * df[name].std()))]\n",
    "    df.drop(drop_rows, axis=0, inplace=True)\n",
    "\n",
    "\n",
    "# Encode a column to a range between normalized_low and normalized_high.\n",
    "def encode_numeric_range(df, name, normalized_low=-1, normalized_high=1,\n",
    "                         data_low=None, data_high=None):\n",
    "    if data_low is None:\n",
    "        data_low = min(df[name])\n",
    "        data_high = max(df[name])\n",
    "\n",
    "    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n",
    "               * (normalized_high - normalized_low) + normalized_low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 150 samples, validate on 38 samples\n",
      "Epoch 1/1000\n",
      " - 0s - loss: 1.0977 - val_loss: 1.0969\n",
      "Epoch 2/1000\n",
      " - 0s - loss: 1.0962 - val_loss: 1.0958\n",
      "Epoch 3/1000\n",
      " - 0s - loss: 1.0948 - val_loss: 1.0943\n",
      "Epoch 4/1000\n",
      " - 0s - loss: 1.0934 - val_loss: 1.0929\n",
      "Epoch 5/1000\n",
      " - 0s - loss: 1.0917 - val_loss: 1.0913\n",
      "Epoch 6/1000\n",
      " - 0s - loss: 1.0901 - val_loss: 1.0896\n",
      "Epoch 7/1000\n",
      " - 0s - loss: 1.0881 - val_loss: 1.0877\n",
      "Epoch 8/1000\n",
      " - 0s - loss: 1.0863 - val_loss: 1.0857\n",
      "Epoch 9/1000\n",
      " - 0s - loss: 1.0838 - val_loss: 1.0834\n",
      "Epoch 10/1000\n",
      " - 0s - loss: 1.0814 - val_loss: 1.0809\n",
      "Epoch 11/1000\n",
      " - 0s - loss: 1.0789 - val_loss: 1.0781\n",
      "Epoch 12/1000\n",
      " - 0s - loss: 1.0758 - val_loss: 1.0750\n",
      "Epoch 13/1000\n",
      " - 0s - loss: 1.0726 - val_loss: 1.0716\n",
      "Epoch 14/1000\n",
      " - 0s - loss: 1.0689 - val_loss: 1.0678\n",
      "Epoch 15/1000\n",
      " - 0s - loss: 1.0651 - val_loss: 1.0636\n",
      "Epoch 16/1000\n",
      " - 0s - loss: 1.0608 - val_loss: 1.0592\n",
      "Epoch 17/1000\n",
      " - 0s - loss: 1.0564 - val_loss: 1.0546\n",
      "Epoch 18/1000\n",
      " - 0s - loss: 1.0517 - val_loss: 1.0494\n",
      "Epoch 19/1000\n",
      " - 0s - loss: 1.0465 - val_loss: 1.0437\n",
      "Epoch 20/1000\n",
      " - 0s - loss: 1.0407 - val_loss: 1.0373\n",
      "Epoch 21/1000\n",
      " - 0s - loss: 1.0346 - val_loss: 1.0302\n",
      "Epoch 22/1000\n",
      " - 0s - loss: 1.0277 - val_loss: 1.0227\n",
      "Epoch 23/1000\n",
      " - 0s - loss: 1.0206 - val_loss: 1.0146\n",
      "Epoch 24/1000\n",
      " - 0s - loss: 1.0127 - val_loss: 1.0061\n",
      "Epoch 25/1000\n",
      " - 0s - loss: 1.0042 - val_loss: 0.9967\n",
      "Epoch 26/1000\n",
      " - 0s - loss: 0.9957 - val_loss: 0.9872\n",
      "Epoch 27/1000\n",
      " - 0s - loss: 0.9864 - val_loss: 0.9773\n",
      "Epoch 28/1000\n",
      " - 0s - loss: 0.9775 - val_loss: 0.9668\n",
      "Epoch 29/1000\n",
      " - 0s - loss: 0.9672 - val_loss: 0.9562\n",
      "Epoch 30/1000\n",
      " - 0s - loss: 0.9570 - val_loss: 0.9451\n",
      "Epoch 31/1000\n",
      " - 0s - loss: 0.9460 - val_loss: 0.9337\n",
      "Epoch 32/1000\n",
      " - 0s - loss: 0.9354 - val_loss: 0.9217\n",
      "Epoch 33/1000\n",
      " - 0s - loss: 0.9242 - val_loss: 0.9094\n",
      "Epoch 34/1000\n",
      " - 0s - loss: 0.9124 - val_loss: 0.8968\n",
      "Epoch 35/1000\n",
      " - 0s - loss: 0.9010 - val_loss: 0.8843\n",
      "Epoch 36/1000\n",
      " - 0s - loss: 0.8888 - val_loss: 0.8714\n",
      "Epoch 37/1000\n",
      " - 0s - loss: 0.8770 - val_loss: 0.8580\n",
      "Epoch 38/1000\n",
      " - 0s - loss: 0.8647 - val_loss: 0.8445\n",
      "Epoch 39/1000\n",
      " - 0s - loss: 0.8525 - val_loss: 0.8315\n",
      "Epoch 40/1000\n",
      " - 0s - loss: 0.8398 - val_loss: 0.8177\n",
      "Epoch 41/1000\n",
      " - 0s - loss: 0.8275 - val_loss: 0.8037\n",
      "Epoch 42/1000\n",
      " - 0s - loss: 0.8145 - val_loss: 0.7897\n",
      "Epoch 43/1000\n",
      " - 0s - loss: 0.8018 - val_loss: 0.7755\n",
      "Epoch 44/1000\n",
      " - 0s - loss: 0.7891 - val_loss: 0.7611\n",
      "Epoch 45/1000\n",
      " - 0s - loss: 0.7761 - val_loss: 0.7470\n",
      "Epoch 46/1000\n",
      " - 0s - loss: 0.7633 - val_loss: 0.7331\n",
      "Epoch 47/1000\n",
      " - 0s - loss: 0.7506 - val_loss: 0.7189\n",
      "Epoch 48/1000\n",
      " - 0s - loss: 0.7378 - val_loss: 0.7052\n",
      "Epoch 49/1000\n",
      " - 0s - loss: 0.7249 - val_loss: 0.6912\n",
      "Epoch 50/1000\n",
      " - 0s - loss: 0.7127 - val_loss: 0.6771\n",
      "Epoch 51/1000\n",
      " - 0s - loss: 0.7001 - val_loss: 0.6638\n",
      "Epoch 52/1000\n",
      " - 0s - loss: 0.6879 - val_loss: 0.6505\n",
      "Epoch 53/1000\n",
      " - 0s - loss: 0.6760 - val_loss: 0.6375\n",
      "Epoch 54/1000\n",
      " - 0s - loss: 0.6645 - val_loss: 0.6252\n",
      "Epoch 55/1000\n",
      " - 0s - loss: 0.6529 - val_loss: 0.6133\n",
      "Epoch 56/1000\n",
      " - 0s - loss: 0.6418 - val_loss: 0.6014\n",
      "Epoch 57/1000\n",
      " - 0s - loss: 0.6315 - val_loss: 0.5899\n",
      "Epoch 58/1000\n",
      " - 0s - loss: 0.6207 - val_loss: 0.5790\n",
      "Epoch 59/1000\n",
      " - 0s - loss: 0.6107 - val_loss: 0.5681\n",
      "Epoch 60/1000\n",
      " - 0s - loss: 0.6010 - val_loss: 0.5575\n",
      "Epoch 61/1000\n",
      " - 0s - loss: 0.5915 - val_loss: 0.5474\n",
      "Epoch 62/1000\n",
      " - 0s - loss: 0.5825 - val_loss: 0.5378\n",
      "Epoch 63/1000\n",
      " - 0s - loss: 0.5737 - val_loss: 0.5288\n",
      "Epoch 64/1000\n",
      " - 0s - loss: 0.5652 - val_loss: 0.5201\n",
      "Epoch 65/1000\n",
      " - 0s - loss: 0.5572 - val_loss: 0.5114\n",
      "Epoch 66/1000\n",
      " - 0s - loss: 0.5496 - val_loss: 0.5032\n",
      "Epoch 67/1000\n",
      " - 0s - loss: 0.5423 - val_loss: 0.4954\n",
      "Epoch 68/1000\n",
      " - 0s - loss: 0.5352 - val_loss: 0.4886\n",
      "Epoch 69/1000\n",
      " - 0s - loss: 0.5281 - val_loss: 0.4815\n",
      "Epoch 70/1000\n",
      " - 0s - loss: 0.5216 - val_loss: 0.4744\n",
      "Epoch 71/1000\n",
      " - 0s - loss: 0.5151 - val_loss: 0.4679\n",
      "Epoch 72/1000\n",
      " - 0s - loss: 0.5095 - val_loss: 0.4616\n",
      "Epoch 73/1000\n",
      " - 0s - loss: 0.5035 - val_loss: 0.4562\n",
      "Epoch 74/1000\n",
      " - 0s - loss: 0.4978 - val_loss: 0.4504\n",
      "Epoch 75/1000\n",
      " - 0s - loss: 0.4925 - val_loss: 0.4450\n",
      "Epoch 76/1000\n",
      " - 0s - loss: 0.4874 - val_loss: 0.4401\n",
      "Epoch 77/1000\n",
      " - 0s - loss: 0.4826 - val_loss: 0.4350\n",
      "Epoch 78/1000\n",
      " - 0s - loss: 0.4778 - val_loss: 0.4305\n",
      "Epoch 79/1000\n",
      " - 0s - loss: 0.4740 - val_loss: 0.4256\n",
      "Epoch 80/1000\n",
      " - 0s - loss: 0.4690 - val_loss: 0.4215\n",
      "Epoch 81/1000\n",
      " - 0s - loss: 0.4647 - val_loss: 0.4174\n",
      "Epoch 82/1000\n",
      " - 0s - loss: 0.4608 - val_loss: 0.4133\n",
      "Epoch 83/1000\n",
      " - 0s - loss: 0.4571 - val_loss: 0.4095\n",
      "Epoch 84/1000\n",
      " - 0s - loss: 0.4532 - val_loss: 0.4058\n",
      "Epoch 85/1000\n",
      " - 0s - loss: 0.4496 - val_loss: 0.4022\n",
      "Epoch 86/1000\n",
      " - 0s - loss: 0.4461 - val_loss: 0.3989\n",
      "Epoch 87/1000\n",
      " - 0s - loss: 0.4429 - val_loss: 0.3956\n",
      "Epoch 88/1000\n",
      " - 0s - loss: 0.4396 - val_loss: 0.3926\n",
      "Epoch 89/1000\n",
      " - 0s - loss: 0.4362 - val_loss: 0.3895\n",
      "Epoch 90/1000\n",
      " - 0s - loss: 0.4332 - val_loss: 0.3864\n",
      "Epoch 91/1000\n",
      " - 0s - loss: 0.4307 - val_loss: 0.3834\n",
      "Epoch 92/1000\n",
      " - 0s - loss: 0.4273 - val_loss: 0.3807\n",
      "Epoch 93/1000\n",
      " - 0s - loss: 0.4244 - val_loss: 0.3780\n",
      "Epoch 94/1000\n",
      " - 0s - loss: 0.4217 - val_loss: 0.3755\n",
      "Epoch 95/1000\n",
      " - 0s - loss: 0.4190 - val_loss: 0.3728\n",
      "Epoch 96/1000\n",
      " - 0s - loss: 0.4164 - val_loss: 0.3703\n",
      "Epoch 97/1000\n",
      " - 0s - loss: 0.4139 - val_loss: 0.3679\n",
      "Epoch 98/1000\n",
      " - 0s - loss: 0.4114 - val_loss: 0.3655\n",
      "Epoch 99/1000\n",
      " - 0s - loss: 0.4090 - val_loss: 0.3631\n",
      "Epoch 100/1000\n",
      " - 0s - loss: 0.4066 - val_loss: 0.3609\n",
      "Epoch 101/1000\n",
      " - 0s - loss: 0.4042 - val_loss: 0.3587\n",
      "Epoch 102/1000\n",
      " - 0s - loss: 0.4021 - val_loss: 0.3565\n",
      "Epoch 103/1000\n",
      " - 0s - loss: 0.3999 - val_loss: 0.3545\n",
      "Epoch 104/1000\n",
      " - 0s - loss: 0.3976 - val_loss: 0.3523\n",
      "Epoch 105/1000\n",
      " - 0s - loss: 0.3954 - val_loss: 0.3502\n",
      "Epoch 106/1000\n",
      " - 0s - loss: 0.3933 - val_loss: 0.3482\n",
      "Epoch 107/1000\n",
      " - 0s - loss: 0.3913 - val_loss: 0.3463\n",
      "Epoch 108/1000\n",
      " - 0s - loss: 0.3894 - val_loss: 0.3445\n",
      "Epoch 109/1000\n",
      " - 0s - loss: 0.3873 - val_loss: 0.3426\n",
      "Epoch 110/1000\n",
      " - 0s - loss: 0.3852 - val_loss: 0.3406\n",
      "Epoch 111/1000\n",
      " - 0s - loss: 0.3834 - val_loss: 0.3388\n",
      "Epoch 112/1000\n",
      " - 0s - loss: 0.3815 - val_loss: 0.3370\n",
      "Epoch 113/1000\n",
      " - 0s - loss: 0.3795 - val_loss: 0.3353\n",
      "Epoch 114/1000\n",
      " - 0s - loss: 0.3777 - val_loss: 0.3335\n",
      "Epoch 115/1000\n",
      " - 0s - loss: 0.3759 - val_loss: 0.3319\n",
      "Epoch 116/1000\n",
      " - 0s - loss: 0.3741 - val_loss: 0.3302\n",
      "Epoch 117/1000\n",
      " - 0s - loss: 0.3724 - val_loss: 0.3285\n",
      "Epoch 118/1000\n",
      " - 0s - loss: 0.3706 - val_loss: 0.3269\n",
      "Epoch 119/1000\n",
      " - 0s - loss: 0.3688 - val_loss: 0.3254\n",
      "Epoch 120/1000\n",
      " - 0s - loss: 0.3672 - val_loss: 0.3237\n",
      "Epoch 121/1000\n",
      " - 0s - loss: 0.3653 - val_loss: 0.3221\n",
      "Epoch 122/1000\n",
      " - 0s - loss: 0.3637 - val_loss: 0.3205\n",
      "Epoch 123/1000\n",
      " - 0s - loss: 0.3621 - val_loss: 0.3189\n",
      "Epoch 124/1000\n",
      " - 0s - loss: 0.3606 - val_loss: 0.3174\n",
      "Epoch 125/1000\n",
      " - 0s - loss: 0.3589 - val_loss: 0.3159\n",
      "Epoch 126/1000\n",
      " - 0s - loss: 0.3571 - val_loss: 0.3144\n",
      "Epoch 127/1000\n",
      " - 0s - loss: 0.3556 - val_loss: 0.3129\n",
      "Epoch 128/1000\n",
      " - 0s - loss: 0.3540 - val_loss: 0.3114\n",
      "Epoch 129/1000\n",
      " - 0s - loss: 0.3526 - val_loss: 0.3100\n",
      "Epoch 130/1000\n",
      " - 0s - loss: 0.3509 - val_loss: 0.3085\n",
      "Epoch 131/1000\n",
      " - 0s - loss: 0.3498 - val_loss: 0.3071\n",
      "Epoch 132/1000\n",
      " - 0s - loss: 0.3480 - val_loss: 0.3058\n",
      "Epoch 133/1000\n",
      " - 0s - loss: 0.3463 - val_loss: 0.3044\n",
      "Epoch 134/1000\n",
      " - 0s - loss: 0.3448 - val_loss: 0.3031\n",
      "Epoch 135/1000\n",
      " - 0s - loss: 0.3434 - val_loss: 0.3017\n",
      "Epoch 136/1000\n",
      " - 0s - loss: 0.3420 - val_loss: 0.3003\n",
      "Epoch 137/1000\n",
      " - 0s - loss: 0.3403 - val_loss: 0.2988\n",
      "Epoch 138/1000\n",
      " - 0s - loss: 0.3391 - val_loss: 0.2974\n",
      "Epoch 139/1000\n",
      " - 0s - loss: 0.3374 - val_loss: 0.2960\n",
      "Epoch 140/1000\n",
      " - 0s - loss: 0.3361 - val_loss: 0.2947\n",
      "Epoch 141/1000\n",
      " - 0s - loss: 0.3349 - val_loss: 0.2935\n",
      "Epoch 142/1000\n",
      " - 0s - loss: 0.3334 - val_loss: 0.2921\n",
      "Epoch 143/1000\n",
      " - 0s - loss: 0.3320 - val_loss: 0.2909\n",
      "Epoch 144/1000\n",
      " - 0s - loss: 0.3303 - val_loss: 0.2896\n",
      "Epoch 145/1000\n",
      " - 0s - loss: 0.3288 - val_loss: 0.2882\n",
      "Epoch 146/1000\n",
      " - 0s - loss: 0.3275 - val_loss: 0.2868\n",
      "Epoch 147/1000\n",
      " - 0s - loss: 0.3262 - val_loss: 0.2855\n",
      "Epoch 148/1000\n",
      " - 0s - loss: 0.3247 - val_loss: 0.2843\n",
      "Epoch 149/1000\n",
      " - 0s - loss: 0.3233 - val_loss: 0.2830\n",
      "Epoch 150/1000\n",
      " - 0s - loss: 0.3220 - val_loss: 0.2818\n",
      "Epoch 151/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 0.3211 - val_loss: 0.2807\n",
      "Epoch 152/1000\n",
      " - 0s - loss: 0.3193 - val_loss: 0.2794\n",
      "Epoch 153/1000\n",
      " - 0s - loss: 0.3180 - val_loss: 0.2779\n",
      "Epoch 154/1000\n",
      " - 0s - loss: 0.3165 - val_loss: 0.2767\n",
      "Epoch 155/1000\n",
      " - 0s - loss: 0.3154 - val_loss: 0.2755\n",
      "Epoch 156/1000\n",
      " - 0s - loss: 0.3139 - val_loss: 0.2742\n",
      "Epoch 157/1000\n",
      " - 0s - loss: 0.3125 - val_loss: 0.2730\n",
      "Epoch 158/1000\n",
      " - 0s - loss: 0.3112 - val_loss: 0.2719\n",
      "Epoch 159/1000\n",
      " - 0s - loss: 0.3098 - val_loss: 0.2706\n",
      "Epoch 160/1000\n",
      " - 0s - loss: 0.3089 - val_loss: 0.2696\n",
      "Epoch 161/1000\n",
      " - 0s - loss: 0.3073 - val_loss: 0.2682\n",
      "Epoch 162/1000\n",
      " - 0s - loss: 0.3060 - val_loss: 0.2669\n",
      "Epoch 163/1000\n",
      " - 0s - loss: 0.3046 - val_loss: 0.2657\n",
      "Epoch 164/1000\n",
      " - 0s - loss: 0.3035 - val_loss: 0.2645\n",
      "Epoch 165/1000\n",
      " - 0s - loss: 0.3022 - val_loss: 0.2633\n",
      "Epoch 166/1000\n",
      " - 0s - loss: 0.3007 - val_loss: 0.2623\n",
      "Epoch 167/1000\n",
      " - 0s - loss: 0.2998 - val_loss: 0.2614\n",
      "Epoch 168/1000\n",
      " - 0s - loss: 0.2985 - val_loss: 0.2598\n",
      "Epoch 169/1000\n",
      " - 0s - loss: 0.2971 - val_loss: 0.2585\n",
      "Epoch 170/1000\n",
      " - 0s - loss: 0.2963 - val_loss: 0.2576\n",
      "Epoch 171/1000\n",
      " - 0s - loss: 0.2946 - val_loss: 0.2562\n",
      "Epoch 172/1000\n",
      " - 0s - loss: 0.2933 - val_loss: 0.2550\n",
      "Epoch 173/1000\n",
      " - 0s - loss: 0.2922 - val_loss: 0.2539\n",
      "Epoch 174/1000\n",
      " - 0s - loss: 0.2909 - val_loss: 0.2529\n",
      "Epoch 175/1000\n",
      " - 0s - loss: 0.2895 - val_loss: 0.2517\n",
      "Epoch 176/1000\n",
      " - 0s - loss: 0.2883 - val_loss: 0.2505\n",
      "Epoch 177/1000\n",
      " - 0s - loss: 0.2873 - val_loss: 0.2495\n",
      "Epoch 178/1000\n",
      " - 0s - loss: 0.2858 - val_loss: 0.2484\n",
      "Epoch 179/1000\n",
      " - 0s - loss: 0.2846 - val_loss: 0.2471\n",
      "Epoch 180/1000\n",
      " - 0s - loss: 0.2836 - val_loss: 0.2459\n",
      "Epoch 181/1000\n",
      " - 0s - loss: 0.2829 - val_loss: 0.2451\n",
      "Epoch 182/1000\n",
      " - 0s - loss: 0.2810 - val_loss: 0.2438\n",
      "Epoch 183/1000\n",
      " - 0s - loss: 0.2797 - val_loss: 0.2426\n",
      "Epoch 184/1000\n",
      " - 0s - loss: 0.2787 - val_loss: 0.2415\n",
      "Epoch 185/1000\n",
      " - 0s - loss: 0.2779 - val_loss: 0.2404\n",
      "Epoch 186/1000\n",
      " - 0s - loss: 0.2763 - val_loss: 0.2394\n",
      "Epoch 187/1000\n",
      " - 0s - loss: 0.2749 - val_loss: 0.2386\n",
      "Epoch 188/1000\n",
      " - 0s - loss: 0.2744 - val_loss: 0.2380\n",
      "Epoch 189/1000\n",
      " - 0s - loss: 0.2732 - val_loss: 0.2366\n",
      "Epoch 190/1000\n",
      " - 0s - loss: 0.2717 - val_loss: 0.2351\n",
      "Epoch 191/1000\n",
      " - 0s - loss: 0.2709 - val_loss: 0.2340\n",
      "Epoch 192/1000\n",
      " - 0s - loss: 0.2695 - val_loss: 0.2330\n",
      "Epoch 193/1000\n",
      " - 0s - loss: 0.2684 - val_loss: 0.2321\n",
      "Epoch 194/1000\n",
      " - 0s - loss: 0.2682 - val_loss: 0.2309\n",
      "Epoch 195/1000\n",
      " - 0s - loss: 0.2659 - val_loss: 0.2302\n",
      "Epoch 196/1000\n",
      " - 0s - loss: 0.2653 - val_loss: 0.2295\n",
      "Epoch 197/1000\n",
      " - 0s - loss: 0.2641 - val_loss: 0.2281\n",
      "Epoch 198/1000\n",
      " - 0s - loss: 0.2629 - val_loss: 0.2268\n",
      "Epoch 199/1000\n",
      " - 0s - loss: 0.2618 - val_loss: 0.2258\n",
      "Epoch 200/1000\n",
      " - 0s - loss: 0.2607 - val_loss: 0.2250\n",
      "Epoch 201/1000\n",
      " - 0s - loss: 0.2596 - val_loss: 0.2241\n",
      "Epoch 202/1000\n",
      " - 0s - loss: 0.2584 - val_loss: 0.2231\n",
      "Epoch 203/1000\n",
      " - 0s - loss: 0.2574 - val_loss: 0.2221\n",
      "Epoch 204/1000\n",
      " - 0s - loss: 0.2562 - val_loss: 0.2209\n",
      "Epoch 205/1000\n",
      " - 0s - loss: 0.2553 - val_loss: 0.2198\n",
      "Epoch 206/1000\n",
      " - 0s - loss: 0.2542 - val_loss: 0.2189\n",
      "Epoch 207/1000\n",
      " - 0s - loss: 0.2534 - val_loss: 0.2181\n",
      "Epoch 208/1000\n",
      " - 0s - loss: 0.2521 - val_loss: 0.2169\n",
      "Epoch 209/1000\n",
      " - 0s - loss: 0.2510 - val_loss: 0.2160\n",
      "Epoch 210/1000\n",
      " - 0s - loss: 0.2499 - val_loss: 0.2153\n",
      "Epoch 211/1000\n",
      " - 0s - loss: 0.2489 - val_loss: 0.2143\n",
      "Epoch 212/1000\n",
      " - 0s - loss: 0.2481 - val_loss: 0.2134\n",
      "Epoch 213/1000\n",
      " - 0s - loss: 0.2469 - val_loss: 0.2123\n",
      "Epoch 214/1000\n",
      " - 0s - loss: 0.2469 - val_loss: 0.2112\n",
      "Epoch 215/1000\n",
      " - 0s - loss: 0.2449 - val_loss: 0.2102\n",
      "Epoch 216/1000\n",
      " - 0s - loss: 0.2438 - val_loss: 0.2096\n",
      "Epoch 217/1000\n",
      " - 0s - loss: 0.2429 - val_loss: 0.2086\n",
      "Epoch 218/1000\n",
      " - 0s - loss: 0.2423 - val_loss: 0.2079\n",
      "Epoch 219/1000\n",
      " - 0s - loss: 0.2414 - val_loss: 0.2066\n",
      "Epoch 220/1000\n",
      " - 0s - loss: 0.2401 - val_loss: 0.2056\n",
      "Epoch 221/1000\n",
      " - 0s - loss: 0.2396 - val_loss: 0.2050\n",
      "Epoch 222/1000\n",
      " - 0s - loss: 0.2379 - val_loss: 0.2040\n",
      "Epoch 223/1000\n",
      " - 0s - loss: 0.2370 - val_loss: 0.2032\n",
      "Epoch 224/1000\n",
      " - 0s - loss: 0.2362 - val_loss: 0.2025\n",
      "Epoch 225/1000\n",
      " - 0s - loss: 0.2352 - val_loss: 0.2015\n",
      "Epoch 226/1000\n",
      " - 0s - loss: 0.2340 - val_loss: 0.2004\n",
      "Epoch 227/1000\n",
      " - 0s - loss: 0.2335 - val_loss: 0.1996\n",
      "Epoch 228/1000\n",
      " - 0s - loss: 0.2327 - val_loss: 0.1987\n",
      "Epoch 229/1000\n",
      " - 0s - loss: 0.2315 - val_loss: 0.1979\n",
      "Epoch 230/1000\n",
      " - 0s - loss: 0.2304 - val_loss: 0.1971\n",
      "Epoch 231/1000\n",
      " - 0s - loss: 0.2297 - val_loss: 0.1967\n",
      "Epoch 232/1000\n",
      " - 0s - loss: 0.2290 - val_loss: 0.1959\n",
      "Epoch 233/1000\n",
      " - 0s - loss: 0.2280 - val_loss: 0.1945\n",
      "Epoch 234/1000\n",
      " - 0s - loss: 0.2269 - val_loss: 0.1936\n",
      "Epoch 235/1000\n",
      " - 0s - loss: 0.2261 - val_loss: 0.1928\n",
      "Epoch 236/1000\n",
      " - 0s - loss: 0.2250 - val_loss: 0.1922\n",
      "Epoch 237/1000\n",
      " - 0s - loss: 0.2247 - val_loss: 0.1920\n",
      "Epoch 238/1000\n",
      " - 0s - loss: 0.2235 - val_loss: 0.1908\n",
      "Epoch 239/1000\n",
      " - 0s - loss: 0.2223 - val_loss: 0.1897\n",
      "Epoch 240/1000\n",
      " - 0s - loss: 0.2220 - val_loss: 0.1887\n",
      "Epoch 241/1000\n",
      " - 0s - loss: 0.2208 - val_loss: 0.1880\n",
      "Epoch 242/1000\n",
      " - 0s - loss: 0.2199 - val_loss: 0.1873\n",
      "Epoch 243/1000\n",
      " - 0s - loss: 0.2194 - val_loss: 0.1865\n",
      "Epoch 244/1000\n",
      " - 0s - loss: 0.2188 - val_loss: 0.1865\n",
      "Epoch 245/1000\n",
      " - 0s - loss: 0.2174 - val_loss: 0.1852\n",
      "Epoch 246/1000\n",
      " - 0s - loss: 0.2164 - val_loss: 0.1842\n",
      "Epoch 247/1000\n",
      " - 0s - loss: 0.2155 - val_loss: 0.1833\n",
      "Epoch 248/1000\n",
      " - 0s - loss: 0.2148 - val_loss: 0.1826\n",
      "Epoch 249/1000\n",
      " - 0s - loss: 0.2142 - val_loss: 0.1818\n",
      "Epoch 250/1000\n",
      " - 0s - loss: 0.2137 - val_loss: 0.1811\n",
      "Epoch 251/1000\n",
      " - 0s - loss: 0.2130 - val_loss: 0.1804\n",
      "Epoch 252/1000\n",
      " - 0s - loss: 0.2116 - val_loss: 0.1797\n",
      "Epoch 253/1000\n",
      " - 0s - loss: 0.2106 - val_loss: 0.1789\n",
      "Epoch 254/1000\n",
      " - 0s - loss: 0.2100 - val_loss: 0.1783\n",
      "Epoch 255/1000\n",
      " - 0s - loss: 0.2091 - val_loss: 0.1776\n",
      "Epoch 256/1000\n",
      " - 0s - loss: 0.2083 - val_loss: 0.1769\n",
      "Epoch 257/1000\n",
      " - 0s - loss: 0.2077 - val_loss: 0.1762\n",
      "Epoch 258/1000\n",
      " - 0s - loss: 0.2077 - val_loss: 0.1752\n",
      "Epoch 259/1000\n",
      " - 0s - loss: 0.2063 - val_loss: 0.1747\n",
      "Epoch 260/1000\n",
      " - 0s - loss: 0.2053 - val_loss: 0.1741\n",
      "Epoch 261/1000\n",
      " - 0s - loss: 0.2047 - val_loss: 0.1732\n",
      "Epoch 262/1000\n",
      " - 0s - loss: 0.2041 - val_loss: 0.1727\n",
      "Epoch 263/1000\n",
      " - 0s - loss: 0.2037 - val_loss: 0.1718\n",
      "Epoch 264/1000\n",
      " - 0s - loss: 0.2025 - val_loss: 0.1712\n",
      "Epoch 265/1000\n",
      " - 0s - loss: 0.2016 - val_loss: 0.1706\n",
      "Epoch 266/1000\n",
      " - 0s - loss: 0.2007 - val_loss: 0.1699\n",
      "Epoch 267/1000\n",
      " - 0s - loss: 0.2001 - val_loss: 0.1692\n",
      "Epoch 268/1000\n",
      " - 0s - loss: 0.1998 - val_loss: 0.1687\n",
      "Epoch 269/1000\n",
      " - 0s - loss: 0.1983 - val_loss: 0.1677\n",
      "Epoch 270/1000\n",
      " - 0s - loss: 0.1980 - val_loss: 0.1671\n",
      "Epoch 271/1000\n",
      " - 0s - loss: 0.1980 - val_loss: 0.1664\n",
      "Epoch 272/1000\n",
      " - 0s - loss: 0.1962 - val_loss: 0.1659\n",
      "Epoch 273/1000\n",
      " - 0s - loss: 0.1966 - val_loss: 0.1664\n",
      "Epoch 274/1000\n",
      " - 0s - loss: 0.1958 - val_loss: 0.1652\n",
      "Epoch 275/1000\n",
      " - 0s - loss: 0.1945 - val_loss: 0.1643\n",
      "Epoch 276/1000\n",
      " - 0s - loss: 0.1938 - val_loss: 0.1632\n",
      "Epoch 277/1000\n",
      " - 0s - loss: 0.1935 - val_loss: 0.1626\n",
      "Epoch 278/1000\n",
      " - 0s - loss: 0.1933 - val_loss: 0.1620\n",
      "Epoch 279/1000\n",
      " - 0s - loss: 0.1918 - val_loss: 0.1615\n",
      "Epoch 280/1000\n",
      " - 0s - loss: 0.1911 - val_loss: 0.1614\n",
      "Epoch 281/1000\n",
      " - 0s - loss: 0.1904 - val_loss: 0.1605\n",
      "Epoch 282/1000\n",
      " - 0s - loss: 0.1907 - val_loss: 0.1594\n",
      "Epoch 283/1000\n",
      " - 0s - loss: 0.1894 - val_loss: 0.1590\n",
      "Epoch 284/1000\n",
      " - 0s - loss: 0.1882 - val_loss: 0.1584\n",
      "Epoch 285/1000\n",
      " - 0s - loss: 0.1877 - val_loss: 0.1577\n",
      "Epoch 286/1000\n",
      " - 0s - loss: 0.1867 - val_loss: 0.1571\n",
      "Epoch 287/1000\n",
      " - 0s - loss: 0.1861 - val_loss: 0.1566\n",
      "Epoch 288/1000\n",
      " - 0s - loss: 0.1861 - val_loss: 0.1563\n",
      "Epoch 289/1000\n",
      " - 0s - loss: 0.1858 - val_loss: 0.1553\n",
      "Epoch 290/1000\n",
      " - 0s - loss: 0.1842 - val_loss: 0.1547\n",
      "Epoch 291/1000\n",
      " - 0s - loss: 0.1838 - val_loss: 0.1543\n",
      "Epoch 292/1000\n",
      " - 0s - loss: 0.1830 - val_loss: 0.1538\n",
      "Epoch 293/1000\n",
      " - 0s - loss: 0.1824 - val_loss: 0.1532\n",
      "Epoch 294/1000\n",
      " - 0s - loss: 0.1816 - val_loss: 0.1525\n",
      "Epoch 295/1000\n",
      " - 0s - loss: 0.1810 - val_loss: 0.1519\n",
      "Epoch 296/1000\n",
      " - 0s - loss: 0.1803 - val_loss: 0.1512\n",
      "Epoch 297/1000\n",
      " - 0s - loss: 0.1799 - val_loss: 0.1506\n",
      "Epoch 298/1000\n",
      " - 0s - loss: 0.1794 - val_loss: 0.1502\n",
      "Epoch 299/1000\n",
      " - 0s - loss: 0.1787 - val_loss: 0.1496\n",
      "Epoch 300/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 0.1780 - val_loss: 0.1491\n",
      "Epoch 301/1000\n",
      " - 0s - loss: 0.1774 - val_loss: 0.1486\n",
      "Epoch 302/1000\n",
      " - 0s - loss: 0.1768 - val_loss: 0.1482\n",
      "Epoch 303/1000\n",
      " - 0s - loss: 0.1762 - val_loss: 0.1474\n",
      "Epoch 304/1000\n",
      " - 0s - loss: 0.1756 - val_loss: 0.1468\n",
      "Epoch 305/1000\n",
      " - 0s - loss: 0.1750 - val_loss: 0.1463\n",
      "Epoch 306/1000\n",
      " - 0s - loss: 0.1743 - val_loss: 0.1458\n",
      "Epoch 307/1000\n",
      " - 0s - loss: 0.1738 - val_loss: 0.1453\n",
      "Epoch 308/1000\n",
      " - 0s - loss: 0.1733 - val_loss: 0.1449\n",
      "Epoch 309/1000\n",
      " - 0s - loss: 0.1729 - val_loss: 0.1442\n",
      "Epoch 310/1000\n",
      " - 0s - loss: 0.1722 - val_loss: 0.1438\n",
      "Epoch 311/1000\n",
      " - 0s - loss: 0.1717 - val_loss: 0.1433\n",
      "Epoch 312/1000\n",
      " - 0s - loss: 0.1708 - val_loss: 0.1426\n",
      "Epoch 313/1000\n",
      " - 0s - loss: 0.1703 - val_loss: 0.1420\n",
      "Epoch 314/1000\n",
      " - 0s - loss: 0.1701 - val_loss: 0.1415\n",
      "Epoch 315/1000\n",
      " - 0s - loss: 0.1694 - val_loss: 0.1412\n",
      "Epoch 316/1000\n",
      " - 0s - loss: 0.1686 - val_loss: 0.1408\n",
      "Epoch 317/1000\n",
      " - 0s - loss: 0.1681 - val_loss: 0.1403\n",
      "Epoch 318/1000\n",
      " - 0s - loss: 0.1679 - val_loss: 0.1396\n",
      "Epoch 319/1000\n",
      " - 0s - loss: 0.1677 - val_loss: 0.1390\n",
      "Epoch 320/1000\n",
      " - 0s - loss: 0.1670 - val_loss: 0.1387\n",
      "Epoch 321/1000\n",
      " - 0s - loss: 0.1660 - val_loss: 0.1383\n",
      "Epoch 322/1000\n",
      " - 0s - loss: 0.1656 - val_loss: 0.1376\n",
      "Epoch 323/1000\n",
      " - 0s - loss: 0.1646 - val_loss: 0.1370\n",
      "Epoch 324/1000\n",
      " - 0s - loss: 0.1654 - val_loss: 0.1367\n",
      "Epoch 325/1000\n",
      " - 0s - loss: 0.1641 - val_loss: 0.1360\n",
      "Epoch 326/1000\n",
      " - 0s - loss: 0.1632 - val_loss: 0.1357\n",
      "Epoch 327/1000\n",
      " - 0s - loss: 0.1627 - val_loss: 0.1355\n",
      "Epoch 328/1000\n",
      " - 0s - loss: 0.1627 - val_loss: 0.1350\n",
      "Epoch 329/1000\n",
      " - 0s - loss: 0.1616 - val_loss: 0.1342\n",
      "Epoch 330/1000\n",
      " - 0s - loss: 0.1618 - val_loss: 0.1337\n",
      "Epoch 331/1000\n",
      " - 0s - loss: 0.1607 - val_loss: 0.1332\n",
      "Epoch 332/1000\n",
      " - 0s - loss: 0.1599 - val_loss: 0.1328\n",
      "Epoch 333/1000\n",
      " - 0s - loss: 0.1595 - val_loss: 0.1326\n",
      "Epoch 334/1000\n",
      " - 0s - loss: 0.1592 - val_loss: 0.1321\n",
      "Epoch 335/1000\n",
      " - 0s - loss: 0.1597 - val_loss: 0.1319\n",
      "Epoch 336/1000\n",
      " - 0s - loss: 0.1578 - val_loss: 0.1309\n",
      "Epoch 337/1000\n",
      " - 0s - loss: 0.1591 - val_loss: 0.1308\n",
      "Epoch 338/1000\n",
      " - 0s - loss: 0.1579 - val_loss: 0.1300\n",
      "Epoch 339/1000\n",
      " - 0s - loss: 0.1568 - val_loss: 0.1298\n",
      "Epoch 340/1000\n",
      " - 0s - loss: 0.1562 - val_loss: 0.1293\n",
      "Epoch 341/1000\n",
      " - 0s - loss: 0.1557 - val_loss: 0.1287\n",
      "Epoch 342/1000\n",
      " - 0s - loss: 0.1551 - val_loss: 0.1283\n",
      "Epoch 343/1000\n",
      " - 0s - loss: 0.1546 - val_loss: 0.1278\n",
      "Epoch 344/1000\n",
      " - 0s - loss: 0.1543 - val_loss: 0.1274\n",
      "Epoch 345/1000\n",
      " - 0s - loss: 0.1537 - val_loss: 0.1270\n",
      "Epoch 346/1000\n",
      " - 0s - loss: 0.1534 - val_loss: 0.1266\n",
      "Epoch 347/1000\n",
      " - 0s - loss: 0.1530 - val_loss: 0.1261\n",
      "Epoch 348/1000\n",
      " - 0s - loss: 0.1524 - val_loss: 0.1257\n",
      "Epoch 349/1000\n",
      " - 0s - loss: 0.1520 - val_loss: 0.1255\n",
      "Epoch 350/1000\n",
      " - 0s - loss: 0.1514 - val_loss: 0.1250\n",
      "Epoch 351/1000\n",
      " - 0s - loss: 0.1511 - val_loss: 0.1244\n",
      "Epoch 352/1000\n",
      " - 0s - loss: 0.1507 - val_loss: 0.1240\n",
      "Epoch 353/1000\n",
      " - 0s - loss: 0.1501 - val_loss: 0.1236\n",
      "Epoch 354/1000\n",
      " - 0s - loss: 0.1495 - val_loss: 0.1233\n",
      "Epoch 355/1000\n",
      " - 0s - loss: 0.1493 - val_loss: 0.1231\n",
      "Epoch 356/1000\n",
      " - 0s - loss: 0.1486 - val_loss: 0.1225\n",
      "Epoch 357/1000\n",
      " - 0s - loss: 0.1490 - val_loss: 0.1219\n",
      "Epoch 358/1000\n",
      " - 0s - loss: 0.1478 - val_loss: 0.1215\n",
      "Epoch 359/1000\n",
      " - 0s - loss: 0.1473 - val_loss: 0.1212\n",
      "Epoch 360/1000\n",
      " - 0s - loss: 0.1470 - val_loss: 0.1208\n",
      "Epoch 361/1000\n",
      " - 0s - loss: 0.1471 - val_loss: 0.1206\n",
      "Epoch 362/1000\n",
      " - 0s - loss: 0.1468 - val_loss: 0.1200\n",
      "Epoch 363/1000\n",
      " - 0s - loss: 0.1457 - val_loss: 0.1196\n",
      "Epoch 364/1000\n",
      " - 0s - loss: 0.1454 - val_loss: 0.1193\n",
      "Epoch 365/1000\n",
      " - 0s - loss: 0.1449 - val_loss: 0.1189\n",
      "Epoch 366/1000\n",
      " - 0s - loss: 0.1444 - val_loss: 0.1185\n",
      "Epoch 367/1000\n",
      " - 0s - loss: 0.1439 - val_loss: 0.1180\n",
      "Epoch 368/1000\n",
      " - 0s - loss: 0.1434 - val_loss: 0.1176\n",
      "Epoch 369/1000\n",
      " - 0s - loss: 0.1434 - val_loss: 0.1173\n",
      "Epoch 370/1000\n",
      " - 0s - loss: 0.1430 - val_loss: 0.1170\n",
      "Epoch 371/1000\n",
      " - 0s - loss: 0.1422 - val_loss: 0.1167\n",
      "Epoch 372/1000\n",
      " - 0s - loss: 0.1418 - val_loss: 0.1162\n",
      "Epoch 373/1000\n",
      " - 0s - loss: 0.1416 - val_loss: 0.1158\n",
      "Epoch 374/1000\n",
      " - 0s - loss: 0.1409 - val_loss: 0.1154\n",
      "Epoch 375/1000\n",
      " - 0s - loss: 0.1410 - val_loss: 0.1151\n",
      "Epoch 376/1000\n",
      " - 0s - loss: 0.1401 - val_loss: 0.1147\n",
      "Epoch 377/1000\n",
      " - 0s - loss: 0.1397 - val_loss: 0.1143\n",
      "Epoch 378/1000\n",
      " - 0s - loss: 0.1402 - val_loss: 0.1141\n",
      "Epoch 379/1000\n",
      " - 0s - loss: 0.1395 - val_loss: 0.1136\n",
      "Epoch 380/1000\n",
      " - 0s - loss: 0.1388 - val_loss: 0.1134\n",
      "Epoch 381/1000\n",
      " - 0s - loss: 0.1382 - val_loss: 0.1129\n",
      "Epoch 382/1000\n",
      " - 0s - loss: 0.1381 - val_loss: 0.1126\n",
      "Epoch 383/1000\n",
      " - 0s - loss: 0.1374 - val_loss: 0.1122\n",
      "Epoch 384/1000\n",
      " - 0s - loss: 0.1369 - val_loss: 0.1118\n",
      "Epoch 385/1000\n",
      " - 0s - loss: 0.1368 - val_loss: 0.1115\n",
      "Epoch 386/1000\n",
      " - 0s - loss: 0.1366 - val_loss: 0.1113\n",
      "Epoch 387/1000\n",
      " - 0s - loss: 0.1359 - val_loss: 0.1109\n",
      "Epoch 388/1000\n",
      " - 0s - loss: 0.1354 - val_loss: 0.1105\n",
      "Epoch 389/1000\n",
      " - 0s - loss: 0.1356 - val_loss: 0.1103\n",
      "Epoch 390/1000\n",
      " - 0s - loss: 0.1346 - val_loss: 0.1098\n",
      "Epoch 391/1000\n",
      " - 0s - loss: 0.1345 - val_loss: 0.1096\n",
      "Epoch 392/1000\n",
      " - 0s - loss: 0.1341 - val_loss: 0.1091\n",
      "Epoch 393/1000\n",
      " - 0s - loss: 0.1341 - val_loss: 0.1089\n",
      "Epoch 394/1000\n",
      " - 0s - loss: 0.1333 - val_loss: 0.1085\n",
      "Epoch 395/1000\n",
      " - 0s - loss: 0.1328 - val_loss: 0.1081\n",
      "Epoch 396/1000\n",
      " - 0s - loss: 0.1329 - val_loss: 0.1079\n",
      "Epoch 397/1000\n",
      " - 0s - loss: 0.1324 - val_loss: 0.1075\n",
      "Epoch 398/1000\n",
      " - 0s - loss: 0.1321 - val_loss: 0.1072\n",
      "Epoch 399/1000\n",
      " - 0s - loss: 0.1316 - val_loss: 0.1071\n",
      "Epoch 400/1000\n",
      " - 0s - loss: 0.1315 - val_loss: 0.1067\n",
      "Epoch 401/1000\n",
      " - 0s - loss: 0.1306 - val_loss: 0.1062\n",
      "Epoch 402/1000\n",
      " - 0s - loss: 0.1304 - val_loss: 0.1060\n",
      "Epoch 403/1000\n",
      " - 0s - loss: 0.1303 - val_loss: 0.1056\n",
      "Epoch 404/1000\n",
      " - 0s - loss: 0.1300 - val_loss: 0.1052\n",
      "Epoch 405/1000\n",
      " - 0s - loss: 0.1296 - val_loss: 0.1049\n",
      "Epoch 406/1000\n",
      " - 0s - loss: 0.1290 - val_loss: 0.1046\n",
      "Epoch 407/1000\n",
      " - 0s - loss: 0.1286 - val_loss: 0.1043\n",
      "Epoch 408/1000\n",
      " - 0s - loss: 0.1285 - val_loss: 0.1040\n",
      "Epoch 409/1000\n",
      " - 0s - loss: 0.1279 - val_loss: 0.1037\n",
      "Epoch 410/1000\n",
      " - 0s - loss: 0.1278 - val_loss: 0.1034\n",
      "Epoch 411/1000\n",
      " - 0s - loss: 0.1272 - val_loss: 0.1032\n",
      "Epoch 412/1000\n",
      " - 0s - loss: 0.1277 - val_loss: 0.1032\n",
      "Epoch 413/1000\n",
      " - 0s - loss: 0.1273 - val_loss: 0.1025\n",
      "Epoch 414/1000\n",
      " - 0s - loss: 0.1264 - val_loss: 0.1021\n",
      "Epoch 415/1000\n",
      " - 0s - loss: 0.1261 - val_loss: 0.1019\n",
      "Epoch 416/1000\n",
      " - 0s - loss: 0.1261 - val_loss: 0.1016\n",
      "Epoch 417/1000\n",
      " - 0s - loss: 0.1258 - val_loss: 0.1015\n",
      "Epoch 418/1000\n",
      " - 0s - loss: 0.1251 - val_loss: 0.1010\n",
      "Epoch 419/1000\n",
      " - 0s - loss: 0.1248 - val_loss: 0.1007\n",
      "Epoch 420/1000\n",
      " - 0s - loss: 0.1242 - val_loss: 0.1005\n",
      "Epoch 421/1000\n",
      " - 0s - loss: 0.1242 - val_loss: 0.1003\n",
      "Epoch 422/1000\n",
      " - 0s - loss: 0.1239 - val_loss: 0.0999\n",
      "Epoch 423/1000\n",
      " - 0s - loss: 0.1236 - val_loss: 0.0996\n",
      "Epoch 424/1000\n",
      " - 0s - loss: 0.1234 - val_loss: 0.0993\n",
      "Epoch 425/1000\n",
      " - 0s - loss: 0.1233 - val_loss: 0.0992\n",
      "Epoch 426/1000\n",
      " - 0s - loss: 0.1231 - val_loss: 0.0988\n",
      "Epoch 427/1000\n",
      " - 0s - loss: 0.1226 - val_loss: 0.0984\n",
      "Epoch 428/1000\n",
      " - 0s - loss: 0.1224 - val_loss: 0.0981\n",
      "Epoch 429/1000\n",
      " - 0s - loss: 0.1217 - val_loss: 0.0979\n",
      "Epoch 430/1000\n",
      " - 0s - loss: 0.1214 - val_loss: 0.0976\n",
      "Epoch 431/1000\n",
      " - 0s - loss: 0.1220 - val_loss: 0.0975\n",
      "Epoch 432/1000\n",
      " - 0s - loss: 0.1214 - val_loss: 0.0971\n",
      "Epoch 433/1000\n",
      " - 0s - loss: 0.1207 - val_loss: 0.0968\n",
      "Epoch 434/1000\n",
      " - 0s - loss: 0.1208 - val_loss: 0.0966\n",
      "Epoch 435/1000\n",
      " - 0s - loss: 0.1199 - val_loss: 0.0963\n",
      "Epoch 436/1000\n",
      " - 0s - loss: 0.1201 - val_loss: 0.0960\n",
      "Epoch 437/1000\n",
      " - 0s - loss: 0.1196 - val_loss: 0.0957\n",
      "Epoch 438/1000\n",
      " - 0s - loss: 0.1195 - val_loss: 0.0956\n",
      "Epoch 439/1000\n",
      " - 0s - loss: 0.1191 - val_loss: 0.0952\n",
      "Epoch 440/1000\n",
      " - 0s - loss: 0.1184 - val_loss: 0.0949\n",
      "Epoch 441/1000\n",
      " - 0s - loss: 0.1183 - val_loss: 0.0947\n",
      "Epoch 442/1000\n",
      " - 0s - loss: 0.1180 - val_loss: 0.0944\n",
      "Epoch 443/1000\n",
      " - 0s - loss: 0.1176 - val_loss: 0.0943\n",
      "Epoch 444/1000\n",
      " - 0s - loss: 0.1173 - val_loss: 0.0940\n",
      "Epoch 445/1000\n",
      " - 0s - loss: 0.1173 - val_loss: 0.0937\n",
      "Epoch 446/1000\n",
      " - 0s - loss: 0.1165 - val_loss: 0.0934\n",
      "Epoch 447/1000\n",
      " - 0s - loss: 0.1170 - val_loss: 0.0934\n",
      "Epoch 448/1000\n",
      " - 0s - loss: 0.1167 - val_loss: 0.0929\n",
      "Epoch 449/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 0.1158 - val_loss: 0.0926\n",
      "Epoch 450/1000\n",
      " - 0s - loss: 0.1157 - val_loss: 0.0924\n",
      "Epoch 451/1000\n",
      " - 0s - loss: 0.1160 - val_loss: 0.0921\n",
      "Epoch 452/1000\n",
      " - 0s - loss: 0.1151 - val_loss: 0.0919\n",
      "Epoch 453/1000\n",
      " - 0s - loss: 0.1151 - val_loss: 0.0916\n",
      "Epoch 454/1000\n",
      " - 0s - loss: 0.1146 - val_loss: 0.0914\n",
      "Epoch 455/1000\n",
      " - 0s - loss: 0.1149 - val_loss: 0.0912\n",
      "Epoch 456/1000\n",
      " - 0s - loss: 0.1142 - val_loss: 0.0909\n",
      "Epoch 457/1000\n",
      " - 0s - loss: 0.1138 - val_loss: 0.0907\n",
      "Epoch 458/1000\n",
      " - 0s - loss: 0.1144 - val_loss: 0.0904\n",
      "Epoch 459/1000\n",
      " - 0s - loss: 0.1142 - val_loss: 0.0904\n",
      "Epoch 460/1000\n",
      " - 0s - loss: 0.1132 - val_loss: 0.0900\n",
      "Epoch 461/1000\n",
      " - 0s - loss: 0.1127 - val_loss: 0.0897\n",
      "Epoch 462/1000\n",
      " - 0s - loss: 0.1129 - val_loss: 0.0896\n",
      "Epoch 463/1000\n",
      " - 0s - loss: 0.1123 - val_loss: 0.0892\n",
      "Epoch 464/1000\n",
      " - 0s - loss: 0.1125 - val_loss: 0.0891\n",
      "Epoch 465/1000\n",
      " - 0s - loss: 0.1121 - val_loss: 0.0888\n",
      "Epoch 466/1000\n",
      " - 0s - loss: 0.1116 - val_loss: 0.0885\n",
      "Epoch 467/1000\n",
      " - 0s - loss: 0.1117 - val_loss: 0.0883\n",
      "Epoch 468/1000\n",
      " - 0s - loss: 0.1113 - val_loss: 0.0881\n",
      "Epoch 469/1000\n",
      " - 0s - loss: 0.1117 - val_loss: 0.0881\n",
      "Epoch 470/1000\n",
      " - 0s - loss: 0.1108 - val_loss: 0.0876\n",
      "Epoch 471/1000\n",
      " - 0s - loss: 0.1109 - val_loss: 0.0877\n",
      "Epoch 472/1000\n",
      " - 0s - loss: 0.1103 - val_loss: 0.0873\n",
      "Epoch 473/1000\n",
      " - 0s - loss: 0.1097 - val_loss: 0.0869\n",
      "Epoch 474/1000\n",
      " - 0s - loss: 0.1096 - val_loss: 0.0870\n",
      "Epoch 475/1000\n",
      " - 0s - loss: 0.1096 - val_loss: 0.0867\n",
      "Epoch 476/1000\n",
      " - 0s - loss: 0.1094 - val_loss: 0.0864\n",
      "Epoch 477/1000\n",
      " - 0s - loss: 0.1087 - val_loss: 0.0861\n",
      "Epoch 478/1000\n",
      " - 0s - loss: 0.1088 - val_loss: 0.0861\n",
      "Epoch 00478: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a1bf2bcf8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "path = \"./data/\"\n",
    "    \n",
    "filename = os.path.join(path,\"iris.csv\")    \n",
    "df = pd.read_csv(filename,na_values=['NA','?'])\n",
    "\n",
    "species = encode_text_index(df,\"species\")\n",
    "x,y = to_xy(df,\"species\")\n",
    "\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(    \n",
    "    x, y, test_size=0.25, random_state=42)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=x.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal'))\n",
    "model.add(Dense(y.shape[1],activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "\n",
    "model.fit(x,y,validation_data=(x_test,y_test),callbacks=[monitor],verbose=2,epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "\n",
    "# Evaluate success using accuracy\n",
    "pred = model.predict(x_test)\n",
    "pred = np.argmax(pred,axis=1) # raw probabilities to chosen class (highest probability)\n",
    "y_compare = np.argmax(y_test,axis=1) \n",
    "score = metrics.accuracy_score(y_compare, pred)\n",
    "print(\"Accuracy score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 398 samples, validate on 100 samples\n",
      "Epoch 1/1000\n",
      " - 0s - loss: 202.9356 - val_loss: 162.2059\n",
      "Epoch 2/1000\n",
      " - 0s - loss: 196.4234 - val_loss: 155.3486\n",
      "Epoch 3/1000\n",
      " - 0s - loss: 190.2756 - val_loss: 152.1142\n",
      "Epoch 4/1000\n",
      " - 0s - loss: 186.0654 - val_loss: 149.0562\n",
      "Epoch 5/1000\n",
      " - 0s - loss: 182.1500 - val_loss: 145.9889\n",
      "Epoch 6/1000\n",
      " - 0s - loss: 178.6752 - val_loss: 142.4402\n",
      "Epoch 7/1000\n",
      " - 0s - loss: 174.1663 - val_loss: 138.4613\n",
      "Epoch 8/1000\n",
      " - 0s - loss: 168.8418 - val_loss: 133.9466\n",
      "Epoch 9/1000\n",
      " - 0s - loss: 163.2540 - val_loss: 128.2846\n",
      "Epoch 10/1000\n",
      " - 0s - loss: 156.0814 - val_loss: 121.5294\n",
      "Epoch 11/1000\n",
      " - 0s - loss: 147.0334 - val_loss: 113.4802\n",
      "Epoch 12/1000\n",
      " - 0s - loss: 136.8134 - val_loss: 105.1051\n",
      "Epoch 13/1000\n",
      " - 0s - loss: 127.1508 - val_loss: 94.1945\n",
      "Epoch 14/1000\n",
      " - 0s - loss: 116.0366 - val_loss: 85.4573\n",
      "Epoch 15/1000\n",
      " - 0s - loss: 107.5330 - val_loss: 76.3530\n",
      "Epoch 16/1000\n",
      " - 0s - loss: 92.2230 - val_loss: 65.5397\n",
      "Epoch 17/1000\n",
      " - 0s - loss: 80.3028 - val_loss: 58.4787\n",
      "Epoch 18/1000\n",
      " - 0s - loss: 71.0316 - val_loss: 53.4106\n",
      "Epoch 19/1000\n",
      " - 0s - loss: 63.4565 - val_loss: 45.0090\n",
      "Epoch 20/1000\n",
      " - 0s - loss: 55.4471 - val_loss: 40.8582\n",
      "Epoch 21/1000\n",
      " - 0s - loss: 51.0089 - val_loss: 37.2317\n",
      "Epoch 22/1000\n",
      " - 0s - loss: 47.8101 - val_loss: 34.4586\n",
      "Epoch 23/1000\n",
      " - 0s - loss: 44.3902 - val_loss: 32.3143\n",
      "Epoch 24/1000\n",
      " - 0s - loss: 39.7309 - val_loss: 31.1407\n",
      "Epoch 25/1000\n",
      " - 0s - loss: 37.8673 - val_loss: 31.4608\n",
      "Epoch 26/1000\n",
      " - 0s - loss: 35.8964 - val_loss: 29.6282\n",
      "Epoch 27/1000\n",
      " - 0s - loss: 34.7558 - val_loss: 28.7571\n",
      "Epoch 28/1000\n",
      " - 0s - loss: 35.0853 - val_loss: 27.3849\n",
      "Epoch 29/1000\n",
      " - 0s - loss: 32.1768 - val_loss: 26.8014\n",
      "Epoch 30/1000\n",
      " - 0s - loss: 31.5435 - val_loss: 26.6988\n",
      "Epoch 31/1000\n",
      " - 0s - loss: 30.4537 - val_loss: 25.7432\n",
      "Epoch 32/1000\n",
      " - 0s - loss: 30.0963 - val_loss: 25.0743\n",
      "Epoch 33/1000\n",
      " - 0s - loss: 29.3578 - val_loss: 26.0326\n",
      "Epoch 34/1000\n",
      " - 0s - loss: 30.4717 - val_loss: 24.6157\n",
      "Epoch 35/1000\n",
      " - 0s - loss: 29.5353 - val_loss: 23.4256\n",
      "Epoch 36/1000\n",
      " - 0s - loss: 27.9915 - val_loss: 22.9680\n",
      "Epoch 37/1000\n",
      " - 0s - loss: 27.9776 - val_loss: 24.4056\n",
      "Epoch 38/1000\n",
      " - 0s - loss: 29.9835 - val_loss: 22.1430\n",
      "Epoch 39/1000\n",
      " - 0s - loss: 26.4280 - val_loss: 23.7675\n",
      "Epoch 40/1000\n",
      " - 0s - loss: 26.4664 - val_loss: 21.2228\n",
      "Epoch 41/1000\n",
      " - 0s - loss: 25.5074 - val_loss: 20.7003\n",
      "Epoch 42/1000\n",
      " - 0s - loss: 24.5425 - val_loss: 20.5317\n",
      "Epoch 43/1000\n",
      " - 0s - loss: 24.3516 - val_loss: 20.7668\n",
      "Epoch 44/1000\n",
      " - 0s - loss: 23.2620 - val_loss: 19.8853\n",
      "Epoch 45/1000\n",
      " - 0s - loss: 22.9880 - val_loss: 20.0195\n",
      "Epoch 46/1000\n",
      " - 0s - loss: 22.8110 - val_loss: 18.9493\n",
      "Epoch 47/1000\n",
      " - 0s - loss: 22.2612 - val_loss: 18.2638\n",
      "Epoch 48/1000\n",
      " - 0s - loss: 22.0082 - val_loss: 18.7902\n",
      "Epoch 49/1000\n",
      " - 0s - loss: 22.8328 - val_loss: 19.3339\n",
      "Epoch 50/1000\n",
      " - 0s - loss: 21.8180 - val_loss: 17.2247\n",
      "Epoch 51/1000\n",
      " - 0s - loss: 21.2287 - val_loss: 16.9466\n",
      "Epoch 52/1000\n",
      " - 0s - loss: 20.4297 - val_loss: 16.6739\n",
      "Epoch 53/1000\n",
      " - 0s - loss: 21.4359 - val_loss: 16.2391\n",
      "Epoch 54/1000\n",
      " - 0s - loss: 20.4440 - val_loss: 16.2151\n",
      "Epoch 55/1000\n",
      " - 0s - loss: 18.9371 - val_loss: 19.1179\n",
      "Epoch 56/1000\n",
      " - 0s - loss: 21.3684 - val_loss: 15.6055\n",
      "Epoch 57/1000\n",
      " - 0s - loss: 18.8556 - val_loss: 16.1648\n",
      "Epoch 58/1000\n",
      " - 0s - loss: 19.4417 - val_loss: 16.7764\n",
      "Epoch 59/1000\n",
      " - 0s - loss: 19.1567 - val_loss: 14.5761\n",
      "Epoch 60/1000\n",
      " - 0s - loss: 18.2057 - val_loss: 15.4962\n",
      "Epoch 61/1000\n",
      " - 0s - loss: 18.2474 - val_loss: 14.1824\n",
      "Epoch 62/1000\n",
      " - 0s - loss: 17.6789 - val_loss: 13.7087\n",
      "Epoch 63/1000\n",
      " - 0s - loss: 16.8836 - val_loss: 13.2414\n",
      "Epoch 64/1000\n",
      " - 0s - loss: 17.3133 - val_loss: 15.3512\n",
      "Epoch 65/1000\n",
      " - 0s - loss: 17.4090 - val_loss: 13.0715\n",
      "Epoch 66/1000\n",
      " - 0s - loss: 16.0159 - val_loss: 13.8305\n",
      "Epoch 67/1000\n",
      " - 0s - loss: 16.1509 - val_loss: 12.3050\n",
      "Epoch 68/1000\n",
      " - 0s - loss: 15.8618 - val_loss: 13.3367\n",
      "Epoch 69/1000\n",
      " - 0s - loss: 15.1753 - val_loss: 11.8937\n",
      "Epoch 70/1000\n",
      " - 0s - loss: 15.2536 - val_loss: 11.8095\n",
      "Epoch 71/1000\n",
      " - 0s - loss: 14.9048 - val_loss: 11.7423\n",
      "Epoch 72/1000\n",
      " - 0s - loss: 14.7058 - val_loss: 11.5616\n",
      "Epoch 73/1000\n",
      " - 0s - loss: 14.8612 - val_loss: 11.4606\n",
      "Epoch 74/1000\n",
      " - 0s - loss: 14.9654 - val_loss: 12.0336\n",
      "Epoch 75/1000\n",
      " - 0s - loss: 14.9202 - val_loss: 10.8448\n",
      "Epoch 76/1000\n",
      " - 0s - loss: 14.0268 - val_loss: 10.6720\n",
      "Epoch 77/1000\n",
      " - 0s - loss: 14.0848 - val_loss: 10.5205\n",
      "Epoch 78/1000\n",
      " - 0s - loss: 13.6022 - val_loss: 10.6460\n",
      "Epoch 79/1000\n",
      " - 0s - loss: 13.3844 - val_loss: 10.1699\n",
      "Epoch 80/1000\n",
      " - 0s - loss: 13.3776 - val_loss: 10.3530\n",
      "Epoch 81/1000\n",
      " - 0s - loss: 13.3535 - val_loss: 10.0147\n",
      "Epoch 82/1000\n",
      " - 0s - loss: 13.7088 - val_loss: 12.6511\n",
      "Epoch 83/1000\n",
      " - 0s - loss: 14.4325 - val_loss: 10.1735\n",
      "Epoch 84/1000\n",
      " - 0s - loss: 13.3884 - val_loss: 9.8203\n",
      "Epoch 85/1000\n",
      " - 0s - loss: 12.5616 - val_loss: 10.4317\n",
      "Epoch 86/1000\n",
      " - 0s - loss: 12.4739 - val_loss: 9.7058\n",
      "Epoch 87/1000\n",
      " - 0s - loss: 12.5309 - val_loss: 9.3593\n",
      "Epoch 88/1000\n",
      " - 0s - loss: 12.4934 - val_loss: 10.0846\n",
      "Epoch 89/1000\n",
      " - 0s - loss: 12.4654 - val_loss: 9.3338\n",
      "Epoch 90/1000\n",
      " - 0s - loss: 12.0685 - val_loss: 9.0250\n",
      "Epoch 91/1000\n",
      " - 0s - loss: 11.8473 - val_loss: 8.9771\n",
      "Epoch 92/1000\n",
      " - 0s - loss: 11.8255 - val_loss: 9.7813\n",
      "Epoch 93/1000\n",
      " - 0s - loss: 11.6053 - val_loss: 8.8287\n",
      "Epoch 94/1000\n",
      " - 0s - loss: 11.6425 - val_loss: 9.1861\n",
      "Epoch 95/1000\n",
      " - 0s - loss: 12.2401 - val_loss: 10.9997\n",
      "Epoch 96/1000\n",
      " - 0s - loss: 12.7558 - val_loss: 9.2414\n",
      "Epoch 97/1000\n",
      " - 0s - loss: 11.6919 - val_loss: 9.0035\n",
      "Epoch 98/1000\n",
      " - 0s - loss: 12.1148 - val_loss: 10.7737\n",
      "Epoch 00098: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a1c7a28d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.stats import zscore\n",
    "\n",
    "path = \"./data/\"\n",
    "\n",
    "filename_read = os.path.join(path,\"auto-mpg.csv\")\n",
    "df = pd.read_csv(filename_read,na_values=['NA','?'])\n",
    "\n",
    "cars = df['name']\n",
    "df.drop('name',1,inplace=True)\n",
    "missing_median(df, 'horsepower')\n",
    "x,y = to_xy(df,\"mpg\")\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(    \n",
    "    x, y, test_size=0.25, random_state=45)\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=x.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal'))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "model.fit(x,y,validation_data=(x_test,y_test),callbacks=[monitor],verbose=2,epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 3.282334566116333\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "pred = model.predict(x_test)\n",
    "\n",
    "# Measure MSE error.  \n",
    "score = metrics.mean_squared_error(pred,y_test)\n",
    "print(\"Final score (RMSE): {}\".format(np.sqrt(score)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]]\n",
      "Build model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jheaton/miniconda3/envs/rga/lib/python3.6/site-packages/ipykernel/__main__.py:27: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/Users/jheaton/miniconda3/envs/rga/lib/python3.6/site-packages/ipykernel/__main__.py:27: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, dropout=0.2, recurrent_dropout=0.2, input_shape=(None, 1))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 0.7001 - acc: 0.5417\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.6948 - acc: 0.5417\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.6890 - acc: 0.7083\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.6846 - acc: 0.7500\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.6712 - acc: 0.7500\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.6672 - acc: 0.7500\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.6615 - acc: 0.7917\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.6596 - acc: 0.7917\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.6512 - acc: 0.7500\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.6339 - acc: 0.7917\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.6366 - acc: 0.7917\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.6288 - acc: 0.7917\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.6395 - acc: 0.7917\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.6002 - acc: 0.7917\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.5950 - acc: 0.7917\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.6322 - acc: 0.7500\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.5997 - acc: 0.7500\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.5963 - acc: 0.7917\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.5547 - acc: 0.7917\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.5547 - acc: 0.7917\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.5651 - acc: 0.7500\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.5658 - acc: 0.7917\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.5269 - acc: 0.7917\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.5427 - acc: 0.7500\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.5111 - acc: 0.7500\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.5153 - acc: 0.7500\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.5075 - acc: 0.7500\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4920 - acc: 0.7500\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4989 - acc: 0.7500\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.5346 - acc: 0.7500\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4853 - acc: 0.7500\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4740 - acc: 0.7500\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4746 - acc: 0.7500\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.5390 - acc: 0.7500\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4752 - acc: 0.7500\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4682 - acc: 0.7500\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4653 - acc: 0.7500\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4578 - acc: 0.7500\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4619 - acc: 0.7500\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4607 - acc: 0.7500\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4643 - acc: 0.7500\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4756 - acc: 0.7500\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4867 - acc: 0.7500\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4465 - acc: 0.7500\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4251 - acc: 0.7917\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4716 - acc: 0.7917\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4245 - acc: 0.7500\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4122 - acc: 0.7917\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4409 - acc: 0.7917\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4548 - acc: 0.7500\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4104 - acc: 0.7917\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4603 - acc: 0.7917\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.3953 - acc: 0.8333\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4114 - acc: 0.7500\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4094 - acc: 0.7917\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3931 - acc: 0.7917\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3982 - acc: 0.7917\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4142 - acc: 0.7917\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.3495 - acc: 0.9167\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.3883 - acc: 0.8333\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4514 - acc: 0.7083\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3716 - acc: 0.8333\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3761 - acc: 0.8333\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4101 - acc: 0.7500\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3578 - acc: 0.7917\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3400 - acc: 0.8333\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.3506 - acc: 0.7500\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3384 - acc: 0.8750\n",
      "Epoch 69/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3411 - acc: 0.8333\n",
      "Epoch 70/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4561 - acc: 0.7083\n",
      "Epoch 71/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2993 - acc: 0.9583\n",
      "Epoch 72/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3042 - acc: 0.9167\n",
      "Epoch 73/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.3114 - acc: 0.9167\n",
      "Epoch 74/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3151 - acc: 0.9167\n",
      "Epoch 75/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.3055 - acc: 0.9583\n",
      "Epoch 76/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3022 - acc: 0.9167\n",
      "Epoch 77/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.5061 - acc: 0.7500\n",
      "Epoch 78/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4716 - acc: 0.8333\n",
      "Epoch 79/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2887 - acc: 0.9167\n",
      "Epoch 80/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4743 - acc: 0.7917\n",
      "Epoch 81/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4199 - acc: 0.8750\n",
      "Epoch 82/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4057 - acc: 0.8333\n",
      "Epoch 83/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2623 - acc: 0.9583\n",
      "Epoch 84/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3071 - acc: 0.9167\n",
      "Epoch 85/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2527 - acc: 0.9583\n",
      "Epoch 86/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2794 - acc: 0.9167\n",
      "Epoch 87/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3932 - acc: 0.8750\n",
      "Epoch 88/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.5138 - acc: 0.7083\n",
      "Epoch 89/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4310 - acc: 0.8333\n",
      "Epoch 90/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4721 - acc: 0.7917\n",
      "Epoch 91/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3942 - acc: 0.8333\n",
      "Epoch 92/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2539 - acc: 0.9167\n",
      "Epoch 93/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2804 - acc: 0.9583\n",
      "Epoch 94/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4010 - acc: 0.8333\n",
      "Epoch 95/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.5016 - acc: 0.7500\n",
      "Epoch 96/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3385 - acc: 0.8333\n",
      "Epoch 97/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2800 - acc: 0.8750\n",
      "Epoch 98/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2756 - acc: 0.9167\n",
      "Epoch 99/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2696 - acc: 0.9167\n",
      "Epoch 100/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3747 - acc: 0.8333\n",
      "Epoch 101/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4813 - acc: 0.7083\n",
      "Epoch 102/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4549 - acc: 0.7500\n",
      "Epoch 103/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.3576 - acc: 0.7917\n",
      "Epoch 104/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3860 - acc: 0.8333\n",
      "Epoch 105/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.3359 - acc: 0.8750\n",
      "Epoch 106/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.3581 - acc: 0.7917\n",
      "Epoch 107/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.3758 - acc: 0.7917\n",
      "Epoch 108/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.3064 - acc: 0.8333\n",
      "Epoch 109/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2484 - acc: 0.9167\n",
      "Epoch 110/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2510 - acc: 0.8750\n",
      "Epoch 111/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2435 - acc: 0.9167\n",
      "Epoch 112/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.3307 - acc: 0.8333\n",
      "Epoch 113/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.3915 - acc: 0.8333\n",
      "Epoch 114/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2160 - acc: 0.9583\n",
      "Epoch 115/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2494 - acc: 0.8750\n",
      "Epoch 116/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.3416 - acc: 0.8333\n",
      "Epoch 117/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2174 - acc: 0.9583\n",
      "Epoch 118/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.3812 - acc: 0.8750\n",
      "Epoch 119/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.3203 - acc: 0.8750\n",
      "Epoch 120/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.3069 - acc: 0.8750\n",
      "Epoch 121/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2136 - acc: 0.9583\n",
      "Epoch 122/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.3135 - acc: 0.8333\n",
      "Epoch 123/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4480 - acc: 0.7917\n",
      "Epoch 124/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2003 - acc: 0.9167\n",
      "Epoch 125/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2756 - acc: 0.9167\n",
      "Epoch 126/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2381 - acc: 0.9167\n",
      "Epoch 127/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2149 - acc: 0.9167\n",
      "Epoch 128/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1975 - acc: 0.9583\n",
      "Epoch 129/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.3295 - acc: 0.8750\n",
      "Epoch 130/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.3093 - acc: 0.8750\n",
      "Epoch 131/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2071 - acc: 0.9583\n",
      "Epoch 132/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2503 - acc: 0.9167\n",
      "Epoch 133/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1742 - acc: 0.9583\n",
      "Epoch 134/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2920 - acc: 0.8750\n",
      "Epoch 135/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2144 - acc: 0.9583\n",
      "Epoch 136/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4589 - acc: 0.8333\n",
      "Epoch 137/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1790 - acc: 0.9583\n",
      "Epoch 138/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2734 - acc: 0.8333\n",
      "Epoch 139/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.6035 - acc: 0.7083\n",
      "Epoch 140/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1692 - acc: 0.9583\n",
      "Epoch 141/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1692 - acc: 0.9167\n",
      "Epoch 142/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.5021 - acc: 0.7917\n",
      "Epoch 143/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1836 - acc: 0.9583\n",
      "Epoch 144/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.3359 - acc: 0.8333\n",
      "Epoch 145/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4263 - acc: 0.7917\n",
      "Epoch 146/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1754 - acc: 0.9583\n",
      "Epoch 147/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2073 - acc: 0.9167\n",
      "Epoch 148/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4516 - acc: 0.7917\n",
      "Epoch 149/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.3561 - acc: 0.8333\n",
      "Epoch 150/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4065 - acc: 0.7917\n",
      "Epoch 151/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2952 - acc: 0.8333\n",
      "Epoch 152/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.3052 - acc: 0.8750\n",
      "Epoch 153/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.3617 - acc: 0.7500\n",
      "Epoch 154/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2489 - acc: 0.8750\n",
      "Epoch 155/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1740 - acc: 0.9167\n",
      "Epoch 156/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2148 - acc: 0.8750\n",
      "Epoch 157/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2614 - acc: 0.8333\n",
      "Epoch 158/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1623 - acc: 0.9583\n",
      "Epoch 159/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2802 - acc: 0.8750\n",
      "Epoch 160/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2953 - acc: 0.8750\n",
      "Epoch 161/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.3485 - acc: 0.8333\n",
      "Epoch 162/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3465 - acc: 0.8333\n",
      "Epoch 163/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2877 - acc: 0.8750\n",
      "Epoch 164/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2452 - acc: 0.8750\n",
      "Epoch 165/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1778 - acc: 1.0000\n",
      "Epoch 166/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1573 - acc: 0.9583\n",
      "Epoch 167/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2084 - acc: 0.9167\n",
      "Epoch 168/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1818 - acc: 0.9583\n",
      "Epoch 169/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2094 - acc: 0.9167\n",
      "Epoch 170/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3056 - acc: 0.9167\n",
      "Epoch 171/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1183 - acc: 1.0000\n",
      "Epoch 172/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1569 - acc: 1.0000\n",
      "Epoch 173/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4671 - acc: 0.7500\n",
      "Epoch 174/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1344 - acc: 1.0000\n",
      "Epoch 175/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1440 - acc: 1.0000\n",
      "Epoch 176/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1788 - acc: 0.9583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 177/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2757 - acc: 0.9167\n",
      "Epoch 178/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.5533 - acc: 0.7500\n",
      "Epoch 179/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2765 - acc: 0.8750\n",
      "Epoch 180/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1300 - acc: 1.0000\n",
      "Epoch 181/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1636 - acc: 1.0000\n",
      "Epoch 182/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2425 - acc: 0.8750\n",
      "Epoch 183/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2069 - acc: 0.9167\n",
      "Epoch 184/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1323 - acc: 1.0000\n",
      "Epoch 185/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2527 - acc: 0.9167\n",
      "Epoch 186/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3874 - acc: 0.8333\n",
      "Epoch 187/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1235 - acc: 1.0000\n",
      "Epoch 188/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4428 - acc: 0.7500\n",
      "Epoch 189/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1016 - acc: 1.0000\n",
      "Epoch 190/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1462 - acc: 1.0000\n",
      "Epoch 191/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2416 - acc: 0.9167\n",
      "Epoch 192/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1706 - acc: 1.0000\n",
      "Epoch 193/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.3495 - acc: 0.8333\n",
      "Epoch 194/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1526 - acc: 1.0000\n",
      "Epoch 195/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1621 - acc: 0.9583\n",
      "Epoch 196/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2470 - acc: 0.9167\n",
      "Epoch 197/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1329 - acc: 1.0000\n",
      "Epoch 198/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1273 - acc: 1.0000\n",
      "Epoch 199/200\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0831 - acc: 1.0000\n",
      "Epoch 200/200\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2255 - acc: 0.9167\n",
      "Predicted classes: {} [1 2 3 2 2 1]\n",
      "Expected classes: {} [1 2 3 2 2 1]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "import numpy as np\n",
    "\n",
    "max_features = 4 # 0,1,2,3 (total of 4)\n",
    "x = [\n",
    "    [[0],[1],[1],[0],[0],[0]],\n",
    "    [[0],[0],[0],[2],[2],[0]],\n",
    "    [[0],[0],[0],[0],[3],[3]],\n",
    "    [[0],[2],[2],[0],[0],[0]],\n",
    "    [[0],[0],[3],[3],[0],[0]],\n",
    "    [[0],[0],[0],[0],[1],[1]]\n",
    "]\n",
    "x = np.array(x,dtype=np.float32)\n",
    "y = np.array([1,2,3,2,3,1],dtype=np.int32)\n",
    "\n",
    "# Convert y2 to dummy variables\n",
    "y2 = np.zeros((y.shape[0], max_features),dtype=np.float32)\n",
    "y2[np.arange(y.shape[0]), y] = 1.0\n",
    "print(y2)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2, input_dim=1))\n",
    "model.add(Dense(4, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x,y2,epochs=200)\n",
    "pred = model.predict(x)\n",
    "predict_classes = np.argmax(pred,axis=1)\n",
    "print(\"Predicted classes: {}\",predict_classes)\n",
    "print(\"Expected classes: {}\",predict_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "def runit(model, inp):\n",
    "    inp = np.array(inp,dtype=np.float32)\n",
    "    pred = model.predict(inp)\n",
    "    return np.argmax(pred[0])\n",
    "\n",
    "print( runit( model, [[[0],[0],[0],[0],[3],[3]]] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rga]",
   "language": "python",
   "name": "conda-env-rga-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
